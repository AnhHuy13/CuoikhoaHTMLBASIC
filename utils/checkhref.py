import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

MIN_HREF_LENGTH = 3
IGNORED_DOMAINS = ["#"]
IGNORED_SCHEMES = ["mailto", "tel"]

def get_line_number(content, tag_string):
    """
    C·ªë g·∫Øng t√¨m s·ªë d√≤ng c·ªßa th·∫ª HTML.
    """
    try:
        lines = content.splitlines()
        short_tag = tag_string[:100]
        for i, line in enumerate(lines):
            if short_tag in line:
                return i + 1
        return 'N/A'
    except:
        return 'N/A'

def check_file_for_missing_links(html_file_path):
    """
    Th·ª±c hi·ªán ki·ªÉm tra c√°c th·∫ª <a> trong m·ªôt file HTML c·ª• th·ªÉ.
    """
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n '{html_file_path}'")
        return []
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file {html_file_path}: {e}")
        return []

    soup = BeautifulSoup(html_content, 'html.parser')
    all_links = soup.find_all('a', href=True)
    potential_issues = []
    
    for link in all_links:
        href = link.get('href', '').strip()
        link_text = link.get_text().strip()
        line_num = get_line_number(html_content, str(link))
        
        issue_type = None

        if href and urlparse(href).scheme in IGNORED_SCHEMES:
            continue

        if not href:
            issue_type = "HREF TR·ªêNG"
        elif href == "#":
            issue_type = "HREF CH·ªà C√ì # (Placeholder)"
        elif len(href) > 0 and len(href) <= MIN_HREF_LENGTH and href not in IGNORED_DOMAINS:
            issue_type = f"HREF R·∫§T NG·∫ÆN (<={MIN_HREF_LENGTH} k√Ω t·ª±)"
        
        if issue_type:
            potential_issues.append({
                "file": html_file_path,
                "type": issue_type,
                "href": href,
                "text": link_text if len(link_text) < 50 else link_text[:47] + "...",
                "line": line_num,
            })
            
    return potential_issues

def check_directory_recursively(root_dir):
    """
    Duy·ªát qua th∆∞ m·ª•c g·ªëc v√† c√°c th∆∞ m·ª•c con ƒë·ªÉ t√¨m v√† ki·ªÉm tra t·∫•t c·∫£ c√°c file HTML.
    """
    total_issues = []
    html_files_checked = 0

    print(f"B·∫Øt ƒë·∫ßu duy·ªát v√† ki·ªÉm tra th∆∞ m·ª•c g·ªëc: {root_dir}\n" + "="*70)

    for dirpath, dirnames, filenames in os.walk(root_dir):
        for filename in filenames:
            if filename.endswith(".html"):
                file_path = os.path.join(dirpath, filename)
                html_files_checked += 1
                
                issues = check_file_for_missing_links(file_path)
                
                if issues:
                    total_issues.extend(issues)
                    print(f"üö® Ph√°t hi·ªán {len(issues)} v·∫•n ƒë·ªÅ trong file: {file_path}")
                else:
                    print(f"‚úÖ File OK: {file_path}")

    print("\n" + "="*70)
    print(f"T·ªîNG K·∫æT: ƒê√£ ki·ªÉm tra {html_files_checked} file HTML.")
    
    if total_issues:
        print(f"\n‚ö†Ô∏è T·ªîNG C·ªòNG {len(total_issues)} V·∫§N ƒê·ªÄ TI·ªÄM ·∫®N C·∫¶N X·ª¨ L√ù:\n")
        
        print(f"{'FILE':<50} | {'D√íNG':<6} | {'LO·∫†I V·∫§N ƒê·ªÄ':<30} | {'HREF':<15} | {'N·ªòI DUNG TEXT':<20}")
        print("-" * 130)
        
        for issue in total_issues:
            short_file = os.path.basename(issue['file'])
            print(f"{short_file:<50} | {issue['line']:<6} | {issue['type']:<30} | {issue['href'][:13]:<15} | {issue['text']:<20}")

    else:
        print("üéâ HO√ÄN T·∫§T! Kh√¥ng t√¨m th·∫•y li√™n k·∫øt tr·ªëng ho·∫∑c placeholder n√†o trong t·∫•t c·∫£ c√°c file HTML.")


if __name__ == "__main__":
    
    
    ROOT_DIRECTORY = "./html"
    
    if os.path.isdir(ROOT_DIRECTORY):
        check_directory_recursively(ROOT_DIRECTORY)
    else:
        print(f"L·ªói: Th∆∞ m·ª•c g·ªëc '{ROOT_DIRECTORY}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")